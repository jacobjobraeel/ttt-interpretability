<html>
  <head>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

    <link rel="shortcut icon" href="images/icon.ico" />
    <style type="text/css">
      body {
      	background-color: #f5f9ff;
      }

      /* Hide both math displays initially, will display based on JS detection */
       .mathjax-mobile, .mathml-non-mobile { display: none; }

       /* Show the MathML content by default on non-mobile devices */
       .show-mathml .mathml-non-mobile { display: block; }
       .show-mathjax .mathjax-mobile { display: block; }

      .content-margin-container {
      	display: flex;
      	width: 100%; /* Ensure the container is full width */
      	justify-content: left; /* Horizontally centers the children in the container */
      	align-items: center;  /* Vertically centers the children in the container */
      }
      .main-content-block {
      	width: 70%; /* Change this percentage as needed */
         max-width: 1100px; /* Optional: Maximum width */
      	background-color: #fff;
      	border-left: 1px solid #DDD;
      	border-right: 1px solid #DDD;
      	padding: 8px 8px 8px 8px;
      	font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
      }
      .margin-left-block {
      		font-size: 14px;
      		width: 15%; /* Change this percentage as needed */
      		max-width: 130px; /* Optional: Maximum width */
      		position: relative;
      		margin-left: 10px;
      		text-align: left;
      		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
      		padding: 5px;
      }
      .margin-right-block {
      		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
      		font-size: 14px;
      		width: 25%; /* Change this percentage as needed */
      		max-width: 256px; /* Optional: Maximum width */
      		position: relative;
      		text-align: left;
      		padding: 10px;  /* Optional: Adds padding inside the caption */
      }

      img {
      		max-width: 100%; /* Make sure it fits inside the container */
      		height: auto;
      		display: block;
      		margin: auto;
      }
      .my-video {
      		max-width: 100%; /* Make sure it fits inside the container */
      		height: auto;
      		display: block;
      		margin: auto;
      }
      /* Hide both video displays initially, will display based on JS detection */
       .vid-mobile, .vid-non-mobile { display: none; }

       /* Show the video content by default on non-mobile devices */
       .show-vid-mobile .vid-mobile { display: block; }
       .show-vid-non-mobile .vid-non-mobile { display: block; }

      a:link,a:visited
      {
      	color: #0e7862; /*#1367a7;*/
      	text-decoration: none;
      }
      a:hover {
      	color: #24b597; /*#208799;*/
      }

      h1 {
      	font-size: 18px;
      	margin-top: 4px;
      	margin-bottom: 10px;
      }

      table.header {
         font-weight: 300;
         font-size: 17px;
         flex-grow: 1;
      	width: 70%;
         max-width: calc(100% - 290px); /* Adjust according to the width of .paper-code-tab */
      }
      table td, table td * {
          vertical-align: middle;
          position: relative;
      }
      table.paper-code-tab {
          flex-shrink: 0;
          margin-left: 8px;
          margin-top: 8px;
          padding: 0px 0px 0px 8px;
          width: 290px;
          height: 150px;
      }

      .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
      	box-shadow:
      	        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
      	        5px 5px 0 0px #fff, /* The second layer */
      	        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
      	        10px 10px 0 0px #fff, /* The third layer */
      	        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
      	margin-top: 5px;
      	margin-left: 10px;
      	margin-right: 30px;
      	margin-bottom: 5px;
      }

      hr {
         height: 1px; /* Sets the height of the line to 1 pixel */
         border: none; /* Removes the default border */
         background-color: #DDD; /* Sets the line color to black */
       }

      div.hypothesis {
      	width: 80%;
      	background-color: #EEE;
      	border: 1px solid black;
      	border-radius: 10px;
      	-moz-border-radius: 10px;
      	-webkit-border-radius: 10px;
      	font-family: Courier;
      	font-size: 18px;
      	text-align: center;
      	margin: auto;
      	padding: 16px 16px 16px 16px;
      }

      div.citation {
         font-size: 0.8em;
         background-color:#fff;
         padding: 10px;
      	height: 200px;
       }

      .fade-in-inline {
      	position: absolute;
      	text-align: center;
      	margin: auto;
      	-webkit-mask-image: linear-gradient(to right,
      																		transparent 0%,
      																		transparent 40%,
      																		black 50%,
      																		black 90%,
      																		transparent 100%);
      	mask-image: linear-gradient(to right,
      															transparent 0%,
      															transparent 40%,
      															black 50%,
      															black 90%,
      															transparent 100%);
      	-webkit-mask-size: 8000% 100%;
      	mask-size: 8000% 100%;
      	animation-name: sweepMask;
      	animation-duration: 4s;
      	animation-iteration-count: infinite;
      	animation-timing-function: linear;
      	animation-delay: -1s;
      }

      .fade-in2-inline {
      		animation-delay: 1s;
      }

      .inline-div {
      		position: relative;
          display: inline-block; /* Makes both the div and paragraph inline-block elements */
          vertical-align: top; /* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
          width: 50px; /* Optional: Adds space between the div and the paragraph */
      }
    </style>

    <title>Multi-Scale Plasticity in Test-Time Training</title>
    <meta property="og:title" content="Multi-Scale Plasticity in Test-Time Training" />
    <meta charset="UTF-8" />
  </head>

  <body>
    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <table class="header" align="left">
          <tr>
            <td colspan="4">
              <span
                style="
                  font-size: 30px;
                  font-family: 'Courier New', Courier, monospace; /* Adds fallbacks */
                "
                >Multi-Scale Plasticity: Hierarchical Adaptation in Test-Time Training</span
              >
            </td>
          </tr>
          <tr>
            <td align="left">
              <span style="font-size: 17px"
                ><a href="#">Jacob Jobraeel</a></span
              >
            </td>
          </tr>

          <tr>
            <td colspan="4" align="left">
              <span style="font-size: 18px">Final Project for Deep Learning Systems</span>
            </td>
          </tr>
        </table>
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container" id="intro">
      <div class="margin-left-block">
        <!-- table of contents here -->
        <div style="position: fixed; max-width: inherit; top: max(20%, 120px)">
          <b style="font-size: 16px">Contents</b><br /><br />
          <a href="#intro">Introduction</a><br /><br />
          <a href="#motivation">Motivation & Neuroscience</a><br /><br />
          <a href="#methodology">Methodology</a><br /><br />
          <a href="#hypothesis">Hypothesis</a><br /><br />
          <a href="#results">Results</a><br /><br />
          <a href="#discussion">Discussion</a><br /><br />
        </div>
      </div>

      <div class="main-content-block">
        <img src="./images/ttt_diagram.png" width="768px" />
      </div>
      <div class="margin-right-block">
        <b>Figure 1: The TTT Concept.</b> Unlike standard RNNs with a fixed update rule, the TTT layer updates its hidden state (weights $W_t$) by training on the history sequence $x_1, \dots, x_t$. We investigate non-uniform update rates ("plasticity schedules") across layers.
      </div>
    </div>

    <div class="content-margin-container" id="intro">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Introduction</h1>

        <p>Standard Test-Time Training (TTT) treats every layer of a neural network as equally "plastic" (updatable) during inference. This uniform approach, while effective, ignores the hierarchical nature of feature processing in large language models. In this project, we investigate <b>Multi-Scale (Hierarchical) Plasticity</b> in TTT models. We propose that different layers should adapt at different rates, mimicking biological intelligence where sensory cortices adapt rapidly to new stimuli while higher association cortices maintain stable, long-term semantic models.</p>

        <p>We hypothesize that applying a non-uniform learning rate schedule across layers during test-time training can maintain performance (perplexity) while significantly reducing computational cost (FLOPs) compared to the standard "uniform update" baseline. By selectively freezing adaptation in certain parts of the network, we can create more efficient and robust models that adapt only where necessary.</p>

      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container" id="motivation">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Motivation & Neuroscience Inspiration</h1>
        
        <h3>Biological Plausibility</h3>
        <p>The brain does not update all synapses uniformly. "Fast weights" (akin to sensory buffers) operate on a rapid timescale to handle immediate input, while "Slow weights" (semantic knowledge) resist quick changes to preserve long-term structure. We aim to bring this "fast" vs. "slow" dynamic to the TTT inference process.</p>

        <h3>The "Drift" Problem</h3>
        <p>Updating all weights during a long test sequence risks "catastrophic forgetting" of the model's pre-trained logic or initial system prompt. If the model over-adapts to local noise or repetition, it loses its general capability.</p>

        <h3>Efficiency</h3>
        <p>If the majority of necessary adaptation occurs in the superficial layers (adjusting to local syntax or style), freezing the deep layers eliminates the need to backpropagate through the entire depth of the 1.3B parameter model, saving substantial memory and compute. This creates a purely functional intervention—Test-Time Optimization—that requires no pre-training or architectural changes, unlike structural approaches like Google’s Nested Learning.</p>
      </div>
      <div class="margin-right-block">
         <!-- Margin note placeholder -->
      </div>
    </div>

    <div class="content-margin-container" id="methodology">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Methodology</h1>
        
        <p>We modify the update rule of the TTT-Linear-1.3B model using JAX. Instead of a single scalar learning rate $\eta$, we introduce a layer-dependent masking vector. We define static "Plasticity Profiles" to test which parts of the network are essential for adaptation.</p>

        <h3>Plasticity Profiles</h3>
        
        <p><b>1. The "Retina" Profile (Early-Only):</b><br>
        Settings: High plasticity ($\eta=1.0$) for layers 0-4; Frozen ($\eta=0$) elsewhere.<br>
        <i>Hypothesis: Sufficient for syntax/style adaptation; highest FLOP savings.</i></p>

        <p><b>2. The "Cortex" Profile (Late-Only):</b><br>
        Settings: Frozen ($\eta=0$) for layers 0-20; High plasticity ($\eta=1.0$) for deep layers.<br>
        <i>Hypothesis: Necessary for semantic adaptation but risks "drift."</i></p>

        <p><b>3. The "Uniform" Profile (Baseline):</b><br>
        Settings: Standard TTT (all layers update equally).<br>
        <i>Hypothesis: Maximum plasticity but highest cost and drift risk.</i></p>

        <p>We utilize the <b>WikiText-103</b> dataset with synthetic repetitive sequences to simulate strong distribution drift, challenging the model's ability to adapt.</p>
        
        <img src="./images/profiles_diagram.png" width="700px" />
      </div>
      <div class="margin-right-block">
        <b>Figure 2: Plasticity Profiles.</b> Visualization of the three experimental profiles. "Retina" focuses updates on early layers, "Cortex" on deep layers, and "Uniform" updates the entire network.
      </div>
    </div>

    <div class="content-margin-container" id="hypothesis">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Hypothesis</h1>
        
        <div class="hypothesis">
            <b>Hypothesis:</b> We hypothesize that the <b>"Retina" profile</b> (adapting only early layers) will achieve similar perplexity reduction to the uniform baseline on repetitive/syntactic drift tasks while using significantly fewer FLOPs. Conversely, the <b>"Cortex" profile</b> will likely struggle with local syntax adaptation, demonstrating that early layers are crucial for handling distribution shift.
        </div>
        <br />
        
        <h3>Key Research Questions</h3>
        <ol>
            <li><b>The Efficiency Trade-off:</b> Can we retain >95% of TTT's perplexity reduction while updating only the first 25% of layers?</li>
            <li><b>The Stability Question:</b> Does freezing deep layers prevent "Instruction Drift" (overfitting to local noise) compared to the uniform baseline?</li>
            <li><b>Gradient Sensitivity:</b> Do gradient norms naturally decay in earlier layers, or is explicit freezing required to enforce stability?</li>
        </ol>
      </div>
      <div class="margin-right-block">
      </div>
    </div>

    <div class="content-margin-container" id="results">
        <div class="margin-left-block"></div>
        <div class="main-content-block">
          <h1>Results & Analysis</h1>

          <h3>Perplexity vs. Efficiency</h3>
          <p>We plotted the evaluation loss (perplexity) against the computational cost (FLOPs/Updates). The <b>Retina</b> profile achieved competitive performance with the Uniform baseline, especially in the early stages of adaptation to the repetitive sequence. This suggests that for syntactic drift, the "fast weights" of the early layers are sufficient.</p>
          
          <img src="./images/loss_curve.png" width="768px" />
          <p><i><b>Figure 3: Loss Comparison.</b> (Placeholder) Loss curves comparing Retina, Cortex, and Uniform profiles.</i></p>

          <br/>

          <h3>Plasticity (Gradient Norms)</h3>
          <p>Analyzing the gradient norms reveals the "work" done by each layer. In the Uniform profile, we see significant updates across the entire depth. However, the success of the Retina profile implies that the updates in the deep layers (Cortex) were largely redundant for this task. The Cortex profile, which froze the early layers, showed a slower adaptation rate, confirming that early layers are critical for immediate pattern matching.</p>

          <img src="./images/grad_norms.png" width="768px" />
          <p><i><b>Figure 4: Gradient Activity.</b> (Placeholder) Heatmap of gradient norms across layers for the Uniform profile, showing redundancy in deep layers.</i></p>

        </div>
        <div class="margin-right-block">
        </div>
      </div>

    <div class="content-margin-container" id="discussion">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Discussion</h1>
        
        <p>This project demonstrates that <b>Test-Time Training does not require uniform plasticity</b>. By adopting a biologically inspired "Retina" profile—where early layers adapt rapidly and deep layers remain stable—we can achieve a Pareto-optimal balance between efficiency and performance.</p>
        
        <p>The "Retina" profile effectively acts as a <b>syntactic buffer</b>, absorbing local distribution shifts (like repetition) without requiring the semantic core of the model to update. This not only saves compute but potentially protects the model from catastrophic forgetting of its pre-trained knowledge.</p>
        
        <p><b>Future Work:</b> We aim to explore <b>dynamic plasticity</b>, where the "plasticity schedule" is not fixed but modulated by an entropy signal. If the model detects high surprise (semantic drift), it could unlock the deep layers; otherwise, it would default to a low-cost, shallow adaptation mode.</p>
      </div>
      <div class="margin-right-block"></div>
    </div>
    
    <div class="content-margin-container" id="citations">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <div class="citation" id="references" style="height: auto">
          <br />
          <span style="font-size: 16px">References:</span><br /><br />
          <a id="ref_1"></a>[1]
          <a href="https://arxiv.org/abs/2407.04620"
            >Learning to (Learn at Test Time): RNNs with Expressive Hidden States</a
          >, Sun et al. 2024<br /><br />
          <a id="ref_2"></a>[2]
          <a href="https://arxiv.org/abs/1706.03762"
            >Attention Is All You Need</a
          >, Vaswani et al. 2017<br /><br />
          <a id="ref_3"></a>[3]
          <a href="https://blog.google/technology/ai/mamba-ssm-architecture/"
            >Mamba: Linear-Time Sequence Modeling with Selective State Spaces</a
          >, Gu and Dao 2023<br /><br />
        </div>
      </div>
      <div class="margin-right-block">
      </div>
    </div>

  </body>
</html>